{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "430fd91e",
   "metadata": {},
   "source": [
    "# Sanjay Regi Philip ADS 509 Assignment 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f79baf9",
   "metadata": {},
   "source": [
    "# Tokenization, Normalization, Descriptive Statistics \n",
    "\n",
    "This notebook holds Assignment 2.1 for Module 2 in ADS 509, Applied Text Mining. Work through this notebook, writing code and answering questions where required. \n",
    "\n",
    "In the previous assignment you put together Twitter data and lyrics data on two artists. In this assignment we explore some of the textual features of those data sets. If, for some reason, you did not complete that previous assignment, data to use for this assignment can be found in the assignment materials section of Blackboard. \n",
    "\n",
    "This assignment asks you to write a short function to calculate some descriptive statistics on a piece of text. Then you are asked to find some interesting and unique statistics on your corpora. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1423c80",
   "metadata": {},
   "source": [
    "### Note to instructor: the data files used for this assignment (twitter followers and lyrics) were subsampled because of performance issues. I attempted to use 200,000+ twitter followers but data cleaning was not finished after 6 hours so I had to use a smaller subsample to complete the assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b921b25",
   "metadata": {},
   "source": [
    "## Import Packages and Find File Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2d096b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "\n",
    "sw = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b555ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add any additional import statements you need here\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "923b5a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# change `data_location` to the location of the folder on your machine.\n",
    "data_location = \"/Users/sanjayregiphilip/OneDrive/DS/ADS 509/Module2/SRegiPhilip-ADS509-Module2/M1 Results\"\n",
    "\n",
    "# These subfolders should still work if you correctly stored the \n",
    "# data from the Module 1 assignment\n",
    "twitter_folder = \"twitter/\"\n",
    "lyrics_folder = \"lyrics/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7563c23",
   "metadata": {},
   "source": [
    "### Define Descriptive Stats Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0018b363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descriptive_stats(tokens, num_tokens = 5, verbose=True) :\n",
    "    \"\"\"\n",
    "        Given a list of tokens, print number of tokens, number of unique tokens, \n",
    "        number of characters, lexical diversity (https://en.wikipedia.org/wiki/Lexical_diversity), \n",
    "        and num_tokens most common tokens. Return a list with the number of tokens, number\n",
    "        of unique tokens, lexical diversity, and number of characters. \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    characters = ''.join(tokens)\n",
    "    \n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = sum(counter.values())\n",
    "    num_unique_tokens = len(counter.keys())\n",
    "    lexical_diversity = num_unique_tokens/num_tokens\n",
    "    num_characters = len(characters)\n",
    "    \n",
    "    if verbose :        \n",
    "        print(f\"There are {num_tokens} tokens in the data.\")\n",
    "        print(f\"There are {num_unique_tokens} unique tokens in the data.\")\n",
    "        print(f\"There are {num_characters} characters in the data.\")\n",
    "        print(f\"The lexical diversity is {lexical_diversity:.3f} in the data.\")\n",
    "    \n",
    "        # print the five most common tokens\n",
    "        print(counter.most_common(5))\n",
    "        \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "59dcf058",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 13 tokens in the data.\n",
      "There are 9 unique tokens in the data.\n",
      "There are 55 characters in the data.\n",
      "The lexical diversity is 0.692 in the data.\n",
      "[('text', 3), ('here', 2), ('example', 2), ('is', 1), ('some', 1)]\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"here is some example text with other example text here in this text\"\"\".split()\n",
    "assert(descriptive_stats(text, verbose=True)[0] == 13)\n",
    "assert(descriptive_stats(text, verbose=False)[1] == 9)\n",
    "assert(abs(descriptive_stats(text, verbose=False)[2] - 0.69) < 0.02)\n",
    "assert(descriptive_stats(text, verbose=False)[3] == 55)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e7e1a2",
   "metadata": {},
   "source": [
    "Q: Why is it beneficial to use assertion statements in your code? \n",
    "\n",
    "A: Assertion code is useful for debugging and allow to ensure that the values being used are indeed what is expected and suitable for the code that it is being executed on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d3bf93e",
   "metadata": {},
   "source": [
    "## Data Input\n",
    "\n",
    "Now read in each of the corpora. For the lyrics data, it may be convenient to store the entire contents of the file to make it easier to inspect the titles individually, as you'll do in the last part of the assignment. In the solution, I stored the lyrics data in a dictionary with two dimensions of keys: artist and song. The value was the file contents. A data frame would work equally well. \n",
    "\n",
    "For the Twitter data, we only need the description field for this assignment. Feel free all the descriptions read it into a data structure. In the solution, I stored the descriptions as a dictionary of lists, with the key being the artist. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def4359f",
   "metadata": {},
   "source": [
    "### Read Lyrics Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37d70801",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Read in the lyrics data\n",
    "import os\n",
    "\n",
    "## Function to remove artist leading name prefix for song file names\n",
    "def remove_filename_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text\n",
    "\n",
    "## Function to read in lyrics from artist directory\n",
    "def read_lyrics(path, prefix, artist_name):\n",
    "    lyrics_dict = defaultdict(dict)\n",
    "    \n",
    "    for filename in os.listdir(path):\n",
    "        name, file_extension = os.path.splitext(filename)\n",
    "        song_name = remove_filename_prefix(name, prefix)\n",
    "        \n",
    "        file = open(path+filename,\"r\", encoding='unicode_escape')\n",
    "        content = file.read()\n",
    "        file.close()\n",
    "        \n",
    "        ## save results to dictionary\n",
    "        lyrics_dict[artist_name][song_name] = content\n",
    "        \n",
    "        \n",
    "    return lyrics_dict\n",
    "\n",
    "\n",
    "# Read in lyrics for Cher\n",
    "path_cher = \"/Users/sanjayregiphilip/OneDrive/DS/ADS 509/Module2/SRegiPhilip-ADS509-Module2/M1 Results/lyrics/cher/\"\n",
    "prefix_cher = \"cher_\"\n",
    "artist_name_cher = \"Cher\"\n",
    "\n",
    "cher_lyrics = read_lyrics(path_cher, prefix_cher, artist_name_cher)\n",
    "cher_songs_complete = cher_lyrics\n",
    "\n",
    "\n",
    "\n",
    "# Read in lyrics for Robyn\n",
    "path_robyn = \"/Users/sanjayregiphilip/OneDrive/DS/ADS 509/Module2/SRegiPhilip-ADS509-Module2/M1 Results/lyrics/robyn/\"\n",
    "prefix_robyn = \"robyn_\"\n",
    "artist_name_robyn = \"Robyn\"\n",
    "\n",
    "robyn_lyrics = read_lyrics(path_robyn, prefix_robyn, artist_name_robyn)\n",
    "robyn_songs_complete = robyn_lyrics\n",
    "\n",
    "\n",
    "# Combine Dictionary for both artists' lyrics\n",
    "lyrics_dict = {\"Cher\": cher_lyrics,\n",
    "               \"Robyn\": robyn_lyrics}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d860af",
   "metadata": {},
   "source": [
    "### Read Twitter Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3a490001",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 624: expected 7 fields, saw 12\\nSkipping line 17506: expected 7 fields, saw 12\\nSkipping line 104621: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 188924: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 301600: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 429936: expected 7 fields, saw 12\\nSkipping line 444405: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 677792: expected 7 fields, saw 12\\nSkipping line 773482: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 818258: expected 7 fields, saw 12\\nSkipping line 895225: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 955213: expected 7 fields, saw 10\\nSkipping line 994827: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 1246039: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 1569117: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2127250: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2335031: expected 7 fields, saw 12\\n'\n",
      "b'Skipping line 2681065: expected 7 fields, saw 10\\n'\n",
      "b'Skipping line 3147696: expected 7 fields, saw 12\\n'\n"
     ]
    }
   ],
   "source": [
    "# Read in the twitter data\n",
    "\n",
    "## Function to read in all twitter follower data in directory\n",
    "def read_twitter(path, artist_name):\n",
    "    twitter_df = pd.read_csv(path, sep=\"\\t\", error_bad_lines=False)\n",
    "    return twitter_df\n",
    "\n",
    "# Read in Cher Follower Data\n",
    "path_cher = \"/Users/sanjayregiphilip/OneDrive/DS/ADS 509/Module2/SRegiPhilip-ADS509-Module2/M1 Results/twitter/cher_followers_data.txt\"\n",
    "\n",
    "cher_twitter_df = read_twitter(path_cher, artist_name_cher)\n",
    "cher_twitter_df = cher_twitter_df.sample(frac=0.05, random_state=1) ## added because of performance issues\n",
    "\n",
    "# Read in Robyn Follower Data\n",
    "path_robyn = \"/Users/sanjayregiphilip/OneDrive/DS/ADS 509/Module2/SRegiPhilip-ADS509-Module2/M1 Results/twitter/robynkonichiwa_followers_data.txt\"\n",
    "\n",
    "robyn_twitter_df = read_twitter(path_robyn, artist_name_robyn)\n",
    "robyn_twitter_df = robyn_twitter_df.sample(frac=0.05, random_state=1) ## added because of performance issues\n",
    "\n",
    "# Create Dictionary for Artist follower description as string in list\n",
    "twitter_dict = {\"Cher\": (cher_twitter_df.description.astype(str).values.tolist()),\n",
    "               \"Robyn\": (robyn_twitter_df.description.astype(str).values.tolist())}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fb2214c2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19582, 7)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cher_twitter_df.shape ## confirm how many followers descriptions are being used due to sampling for performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a5f3b12",
   "metadata": {},
   "source": [
    "## Data Cleaning\n",
    "\n",
    "Now clean and tokenize your data. Remove punctuation chacters (available in the `punctuation` object in the `string` library), split on whitespace, fold to lowercase, and remove stopwords. Store your cleaned data, which must be accessible as an interable for `descriptive_stats`, in new objects or in new columns in your data frame. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "71c73d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "punctuation = set(punctuation) # speeds up comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "662af63a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve follower data for each artist\n",
    "twitter_cher = twitter_dict['Cher']\n",
    "twitter_robyn = twitter_dict['Robyn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f0825239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold to lowercase\n",
    "for index, item in enumerate(twitter_cher):\n",
    "    twitter_cher[index] = item.casefold()\n",
    "    \n",
    "for index, item in enumerate(twitter_robyn):\n",
    "    twitter_robyn[index] = item.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7ecf5e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "for index, item in enumerate(twitter_cher):\n",
    "    twitter_cher[index] = re.sub(r'[^\\w\\s]','',item)\n",
    "    \n",
    "for index, item in enumerate(twitter_robyn):\n",
    "    twitter_robyn[index] = re.sub(r'[^\\w\\s]','',item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7e0bb26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split on whitespace\n",
    "for index, item in enumerate(twitter_cher):\n",
    "    twitter_cher[index] = item.split()\n",
    "    \n",
    "for index, item in enumerate(twitter_robyn):\n",
    "    twitter_robyn[index] = item.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a260b98b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns all words to single list\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "twitter_cher = flatten(twitter_cher)\n",
    "twitter_robyn = flatten(twitter_robyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "109822ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "twitter_cher = [word for word in twitter_cher if not word in stopwords.words()]\n",
    "twitter_robyn = [word for word in twitter_robyn if not word in stopwords.words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e97cabc",
   "metadata": {},
   "source": [
    "### Data Cleaning for Lyrics\n",
    "#### Folds to lowercase, removes punctuation, splits on whitespace, removes stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4f031d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "cher_lyrics = lyrics_dict['Cher']['Cher']\n",
    "robyn_lyrics = lyrics_dict['Robyn']['Robyn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6edd305b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cher_lyrics=list(cher_lyrics.values())\n",
    "robyn_lyrics=list(robyn_lyrics.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1c3e7edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold to lowercase\n",
    "for index, item in enumerate(cher_lyrics):\n",
    "    cher_lyrics[index] = item.casefold()\n",
    "\n",
    "for index, item in enumerate(robyn_lyrics):\n",
    "    robyn_lyrics[index] = item.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b57d556e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "for index, item in enumerate(cher_lyrics):\n",
    "    cher_lyrics[index] = re.sub(r'[^\\w\\s]','',item)\n",
    "    \n",
    "for index, item in enumerate(robyn_lyrics):\n",
    "    robyn_lyrics[index] = re.sub(r'[^\\w\\s]','',item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "392d86a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split on whitespace\n",
    "for index, item in enumerate(cher_lyrics):\n",
    "    cher_lyrics[index] = item.split()\n",
    "    \n",
    "for index, item in enumerate(robyn_lyrics):\n",
    "    robyn_lyrics[index] = item.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "14cb7a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns all words to single list\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "lyrics_cher = flatten(cher_lyrics)\n",
    "lyrics_robyn = flatten(robyn_lyrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84f761da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "lyrics_cher = [word for word in lyrics_cher if not word in stopwords.words()]\n",
    "lyrics_robyn = [word for word in lyrics_robyn if not word in stopwords.words()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2dd0179",
   "metadata": {},
   "source": [
    "## Basic Descriptive Statistics\n",
    "\n",
    "Call your `descriptive_stats` function on both your lyrics data and your twitter data and for both artists (four total calls). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "60b97002",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Stats for Cher's Followers\n",
      "There are 79153 tokens in the data.\n",
      "There are 25437 unique tokens in the data.\n",
      "There are 478071 characters in the data.\n",
      "The lexical diversity is 0.321 in the data.\n",
      "[('nan', 9810), ('love', 1191), ('life', 703), ('music', 487), ('follow', 328)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[79153, 25437, 0.32136495142319305, 478071]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Descriptive Stats for Cher's Followers\")\n",
    "descriptive_stats(twitter_cher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d53ce073",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Stats for Cher's Lyrics\n",
      "There are 6608 tokens in the data.\n",
      "There are 3924 unique tokens in the data.\n",
      "There are 41707 characters in the data.\n",
      "The lexical diversity is 0.594 in the data.\n",
      "[('nan', 823), ('music', 83), ('love', 47), ('lover', 32), ('life', 29)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[6608, 3924, 0.5938256658595642, 41707]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Descriptive Stats for Cher's Lyrics\")\n",
    "descriptive_stats(twitter_robyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a514873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Stats for Cher's Lyrics\n",
      "There are 1768 tokens in the data.\n",
      "There are 602 unique tokens in the data.\n",
      "There are 10001 characters in the data.\n",
      "The lexical diversity is 0.340 in the data.\n",
      "[('bang', 88), ('love', 60), ('baby', 37), ('heart', 33), ('youre', 32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1768, 602, 0.3404977375565611, 10001]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Descriptive Stats for Cher's Lyrics\")\n",
    "descriptive_stats(lyrics_cher)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d36caea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Stats for Robyn's Lyrics\n",
      "There are 2440 tokens in the data.\n",
      "There are 794 unique tokens in the data.\n",
      "There are 13736 characters in the data.\n",
      "The lexical diversity is 0.325 in the data.\n",
      "[('baby', 50), ('time', 36), ('youre', 35), ('love', 35), ('wow', 32)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2440, 794, 0.3254098360655738, 13736]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Descriptive Stats for Robyn's Lyrics\")\n",
    "descriptive_stats(lyrics_robyn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46294409",
   "metadata": {},
   "source": [
    "Q: How do you think the \"top 5 words\" would be different if we left stopwords in the data? \n",
    "\n",
    "A: Stopwords are likely to be the most common of words and would likely show in any top 5 words lists unless filtered out explicitly.\n",
    "\n",
    "---\n",
    "\n",
    "Q: What were your prior beliefs about the lexical diversity between the artists? Does the difference (or lack thereof) in lexical diversity between the artists conform to your prior beliefs? \n",
    "\n",
    "A: As both of these artists are known as \"pop artists\", I am not surprised by what seems to be a relatively low lexical diversity. Pop music in general is known to be repetitive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4e1ac1",
   "metadata": {},
   "source": [
    "\n",
    "## Specialty Statistics\n",
    "\n",
    "The descriptive statistics we have calculated are quite generic. You will now calculate a handful of statistics tailored to these data.\n",
    "\n",
    "1. Ten most common emojis by artist in the twitter descriptions.\n",
    "1. Ten most common hashtags by artist in the twitter descriptions.\n",
    "1. Five most common words in song titles by artist. \n",
    "1. For each artist, a histogram of song lengths (in terms of number of tokens) \n",
    "\n",
    "We can use the `emoji` library to help us identify emojis and you have been given a function to help you.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "753a5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(emoji.is_emoji(\"わ\"))\n",
    "assert(not emoji.is_emoji(\":-)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986fc4c0",
   "metadata": {},
   "source": [
    "### Emojis \n",
    "\n",
    "What are the ten most common emojis by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "01ce1549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(1 for token in twitter_cher if (emoji.is_emoji(token)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bab9b770",
   "metadata": {},
   "source": [
    "### Hashtags\n",
    "\n",
    "What are the ten most common hashtags by artist in the twitter descriptions? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f21d5",
   "metadata": {},
   "source": [
    "### Song Titles\n",
    "\n",
    "What are the five most common words in song titles by artist? The song titles should be on the first line of the lyrics pages, so if you have kept the raw file contents around, you will not need to re-read the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c6f7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### I re-read the data to ensure no important information was lost earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e4c80d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_cher = \"/Users/sanjayregiphilip/OneDrive/DS/ADS 509/Module2/SRegiPhilip-ADS509-Module2/M1 Results/lyrics/cher/\"\n",
    "\n",
    "path_robyn = \"/Users/sanjayregiphilip/OneDrive/DS/ADS 509/Module2/SRegiPhilip-ADS509-Module2/M1 Results/lyrics/robyn/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "bb69b36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to read first line of each song .txt file to gather name\n",
    "# This was needed because we originally extracted song name from the filename rather than the first line\n",
    "\n",
    "def read_song_names(path, artist_name):\n",
    "    \n",
    "    song_name = list()\n",
    "    type(song_name)\n",
    "    for filename in os.listdir(path):\n",
    "        file = open(path+filename,\"r\", encoding='unicode_escape')\n",
    "        song_name.append(str(file.readline()))\n",
    "        file.close()\n",
    "        \n",
    "    return song_name\n",
    "\n",
    "cher_song_names = read_song_names(path_cher, artist_name_cher)\n",
    "robyn_song_names = read_song_names(path_robyn, artist_name_robyn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "1d1cb055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fold to lowercase\n",
    "for index, item in enumerate(cher_song_names):\n",
    "    cher_song_names[index] = item.casefold()\n",
    "\n",
    "for index, item in enumerate(robyn_song_names):\n",
    "    robyn_song_names[index] = item.casefold()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cfa3e5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove punctuation\n",
    "for index, item in enumerate(cher_song_names):\n",
    "    cher_song_names[index] = re.sub(r'[^\\w\\s]','',item)\n",
    "    \n",
    "for index, item in enumerate(cher_song_names):\n",
    "    robyn_song_names[index] = re.sub(r'[^\\w\\s]','',item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "038ddb78",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split on whitespace\n",
    "for index, item in enumerate(cher_song_names):\n",
    "    cher_song_names[index] = str(item).split()\n",
    "    \n",
    "for index, item in enumerate(robyn_lyrics):\n",
    "    robyn_song_names[index] = str(item).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1c31d9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns all words to single list\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "cher_song_names = flatten(cher_song_names)\n",
    "robyn_song_names = flatten(robyn_song_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9e7e64e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stopwords\n",
    "cher_song_names = [word for word in cher_song_names if not word in stopwords.words()]\n",
    "robyn_song_names = [word for word in robyn_song_names if not word in stopwords.words()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4b110fbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Stats for Cher's Song Names Words\n",
      "There are 55 tokens in the data.\n",
      "There are 50 unique tokens in the data.\n",
      "There are 1398 characters in the data.\n",
      "The lexical diversity is 0.909 in the data.\n",
      "[('body', 2), ('heart', 2), ('每每每每每每', 2), ('cher_bodytobodyhearttohearttxtilocblob每每每每每每cher_bornwiththehungertxtilocblob每每每每每每', 2), ('bang', 2)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[55, 50, 0.9090909090909091, 1398]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Descriptive Stats for Cher's Song Names Words\")\n",
    "descriptive_stats(cher_song_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d07ea02c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descriptive Stats for Robyn's Song Names Words\n",
      "There are 7062 tokens in the data.\n",
      "There are 1085 unique tokens in the data.\n",
      "There are 49747 characters in the data.\n",
      "The lexical diversity is 0.154 in the data.\n",
      "[(\"'you',\", 355), (\"'the',\", 264), (\"'i',\", 204), (\"'and',\", 166), (\"'to',\", 128)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[7062, 1085, 0.15363919569527046, 49747]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Descriptive Stats for Robyn's Song Names Words\")\n",
    "descriptive_stats(robyn_song_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dd4fd71",
   "metadata": {},
   "source": [
    "### Song Lengths\n",
    "\n",
    "For each artist, a histogram of song lengths (in terms of number of tokens). If you put the song lengths in a data frame with an artist column, matplotlib will make the plotting quite easy. An example is given to help you out. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fde9ebb",
   "metadata": {},
   "source": [
    "Since the lyrics may be stored with carriage returns or tabs, it may be useful to have a function that can collapse whitespace, using regular expressions, and be used for splitting. \n",
    "\n",
    "Q: What does the regular expression `'\\s+'` match on? \n",
    "\n",
    "A: It will help find any examples where there is more than 1 whitespace character.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "712d603a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Method not used - was originally intended to count lyric length\n",
    "def count_tokens(tokens, num_tokens = 5, verbose=True) :\n",
    "\n",
    "    counter = Counter(tokens)\n",
    "    characters = ''.join(tokens)\n",
    "    \n",
    "    # Fill in the correct values here. \n",
    "    num_tokens = sum(counter.values())\n",
    "    num_unique_tokens = len(counter.keys())\n",
    "    lexical_diversity = num_unique_tokens/num_tokens\n",
    "    num_characters = len(characters)\n",
    "    \n",
    "    return([num_tokens, num_unique_tokens,\n",
    "            lexical_diversity,\n",
    "            num_characters])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fea73b63",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cher_songs_complete = cher_songs_complete['Cher']\n",
    "robyn_songs_complete = robyn_songs_complete['Robyn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dacdc025",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Artist\n",
       "Cher     AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Robyn    AxesSubplot(0.125,0.125;0.775x0.755)\n",
       "Name: Count, dtype: object"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cher_lyrics_count = pd.DataFrame(columns=['Song Name', 'Count'])\n",
    "\n",
    "for k,v in cher_songs_complete.items():\n",
    "    cher_lyrics_count = cher_lyrics_count.append({'Song Name': k, 'Count': len(v)}, ignore_index = True)\n",
    "    \n",
    "    \n",
    "robyn_lyrics_count = pd.DataFrame(columns=['Song Name', 'Count'])\n",
    "\n",
    "for k,v in robyn_songs_complete.items():\n",
    "    robyn_lyrics_count = robyn_lyrics_count.append({'Song Name': k, 'Count': len(v)}, ignore_index = True)\n",
    "\n",
    "    \n",
    "cher_lyrics_count['Artist'] = \"Cher\"\n",
    "robyn_lyrics_count['Artist'] = 'Robyn'\n",
    "\n",
    "lyrics_count = pd.concat([cher_lyrics_count, robyn_lyrics_count], axis=0)\n",
    "\n",
    "lyrics_count.groupby('Artist')['Count'].plot(kind=\"hist\",density=True,alpha=0.5,legend=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
